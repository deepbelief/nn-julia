{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple neural network in julia\n",
    "Based on Andrew Ng's Machine Learning course in Coursera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using MNIST # hand written digits\n",
    "\n",
    "trainX, trainY = traindata();\n",
    "testX, testY = testdata();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# limit the dataset for demonstration purpose\n",
    "trainX = trainX[:,1:5000]\n",
    "trainY = trainY[1:5000]\n",
    "\n",
    "testX = testX[:,1:1000]\n",
    "testY = testY[1:1000]\n",
    "\n",
    "trainY = round(Int, trainY) + 1 # convert to integer and allow indexing later\n",
    "testY = round(Int, testY) + 1 # convert to integer and allow indexing later\n",
    "\n",
    "print(\"Number of features: \", size(trainX,1), \" (28x28 grey scale image)\")\n",
    "print(\"\\n\")\n",
    "print(\"Number of training examples: \", size(trainX,2))\n",
    "print(\"\\n\")\n",
    "print(\"Number of testing examples: \", size(testX,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# display random images from the training set\n",
    "using PyPlot;\n",
    "\n",
    "for i = 1:25\n",
    "    PyPlot.subplot(5,5,i)\n",
    "    PyPlot.axis(\"off\")\n",
    "    PyPlot.imshow(reshape(trainX'[rand(1:size(trainX',1)),:],28,28), cmap=PyPlot.ColorMap(\"gray\"))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##adding bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = size(trainX,2) # number of training examples\n",
    "X = trainX'\n",
    "y = trainY'\n",
    "a1 = [ones(m, 1) X] # adding bias units set to 1\n",
    "\n",
    "mtest = size(testX,2) # number of testing examples\n",
    "Xtest = testX'\n",
    "ytest = testY'\n",
    "a1test = [ones(mtest, 1) Xtest] # adding bias units set to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_layer_size = size(X,2) #input layer\n",
    "hidden_layer_size = 20 # hidden layer\n",
    "num_labels = 10 # output layer, number of digits for classification, 0-9\n",
    "\n",
    "lambda = 1 # regularization parameter\n",
    "iterations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare target for multiclass classification\n",
    "Y = zeros(m,num_labels)\n",
    "for i = 1:m\n",
    "    Y[i, y[i]] = 1\n",
    "end\n",
    "\n",
    "Ytest = zeros(mtest,num_labels)\n",
    "for i = 1:mtest\n",
    "    Ytest[i, ytest[i]] = 1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialize weights between input and hidden layers (Theta1), and between hidden and output layers (Theta2)\n",
    "epsilon_init = 0.12;\n",
    "Theta1 = rand(hidden_layer_size,input_layer_size+1) * 2 * epsilon_init - epsilon_init\n",
    "Theta2 = rand(num_labels,hidden_layer_size+1) * 2 * epsilon_init - epsilon_init\n",
    "Theta = vcat(Theta1[:], Theta2[:]) #unroll weights as required by optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zigmoid(z) =  1.0 ./ (1.0 + exp(-z)); # sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gizmoid(z) = zigmoid(z).*(1-zigmoid(z)); # derivative of sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cost function\n",
    "function nnCostFn(Theta)\n",
    "    Theta1 = reshape(Theta[1:hidden_layer_size * (input_layer_size + 1)], hidden_layer_size, (input_layer_size + 1))\n",
    "    Theta2 = reshape(Theta[(1 + (hidden_layer_size * (input_layer_size + 1))):end], num_labels, (hidden_layer_size + 1))\n",
    "\n",
    "    # forward pass\n",
    "    z2 = *(Theta1, a1') # input to hidden layer\n",
    "    a2 = zigmoid(z2) # hidden layer activations\n",
    "    a2 = [ones(m) a2'] # adding bias units\n",
    "    z3 = *(Theta2, a2') # input to output layer\n",
    "    a3 = zigmoid(z3') # output layer activations\n",
    "    \n",
    "    J =  1/m * sum((-Y .* log(a3))  -   ((1 - Y) .* (log(1 - a3)))) # cross-entropy cost function\n",
    "    Theta1Sq = Theta1[:,2:end].^2 # exclude bias\n",
    "    Theta2Sq = Theta2[:,2:end].^2 # exclude bias\n",
    "    J = J + lambda/(2*m) * (sum(Theta1Sq) +sum(Theta2Sq)) # regularized cost function\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculating gradients\n",
    "function gradient(Theta)\n",
    "    Theta1 = reshape(Theta[1:hidden_layer_size * (input_layer_size + 1)], hidden_layer_size, (input_layer_size + 1));\n",
    "    Theta2 = reshape(Theta[(1 + (hidden_layer_size * (input_layer_size + 1))):end], num_labels, (hidden_layer_size + 1));\n",
    "\n",
    "    # forward pass\n",
    "    z2 = *(Theta1, a1') # input to hidden layer\n",
    "    a2 = zigmoid(z2) # hidden layer activations\n",
    "    a2 = [ones(m) a2'] # adding bias units\n",
    "    z3 = *(Theta2, a2') # input to output layer\n",
    "    a3 = zigmoid(z3') # output layer activations  \n",
    "    \n",
    "    # transpose matrices\n",
    "    a1t = a1';\n",
    "    a2t = a2';\n",
    "    a3t = a3';\n",
    "    Yt  = Y';\n",
    "\n",
    "    # backpropagation\n",
    "    # calculate errors\n",
    "    delta3 = a3t - Yt;\n",
    "    z2t = vcat(ones(1,m), *(Theta1,a1t))\n",
    "    delta2 = *(Theta2',delta3).*gizmoid(z2t);\n",
    "\n",
    "    # calculate gradients\n",
    "    Theta1_grad = *(delta2[2:end,:],a1t');\n",
    "    Theta2_grad = *(delta3,a2t');\n",
    "\n",
    "    # normalize and regularize gradients\n",
    "    Theta2_grad = Theta2_grad/m + (lambda/m)* hcat(zeros(size(Theta2, 1), 1), Theta2[:,2:end]);\n",
    "    Theta1_grad = Theta1_grad/m + (lambda/m)* hcat(zeros(size(Theta1, 1), 1), Theta1[:,2:end]);\n",
    "    \n",
    "    # unroll\n",
    "    grad = vcat(Theta1_grad[:], Theta2_grad[:])\n",
    "    end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradient check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate gradients using auto differentiation and compare with gradients calculated using backpropagation\n",
    "# this check is computationally expensive, therefore use on small networks only\n",
    "# using ForwardDiff\n",
    "# g = a1 -> ForwardDiff.gradient(nnCostFn, Theta)\n",
    "# gradient(Theta) - g(Theta) # difference should be negligible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optimization using conjugate gradient (CG) descent algorithm in scipy imported from Python\n",
    "# using cost function, gradient function and randomly initialized weights\n",
    "using PyCall\n",
    "@pyimport scipy.optimize as scop\n",
    "@time ThetaTrained = scop.fmin_cg(nnCostFn, Theta, fprime=gradient, maxiter=iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function predict(ThetaTrained)\n",
    "    Theta1 = reshape(ThetaTrained[1:hidden_layer_size * (input_layer_size + 1)], hidden_layer_size, (input_layer_size + 1));\n",
    "    Theta2 = reshape(ThetaTrained[(1 + (hidden_layer_size * (input_layer_size + 1))):end], num_labels, (hidden_layer_size + 1));\n",
    "\n",
    "    # forward pass\n",
    "    z2 = *(Theta1, a1test') # input to hidden layer\n",
    "    a2 = zigmoid(z2) # hidden layer activations\n",
    "    a2 = [ones(mtest) a2'] # adding bias units\n",
    "    z3 = *(Theta2, a2') # input to output layer\n",
    "    a3 = zigmoid(z3') # output layer activations \n",
    "\n",
    "    val, p = findmax(a3', 1) # find index holding the highest probability\n",
    "    (sum(p%10 .== ytest%10) /mtest)*100 # returns prediction accuracy\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict(ThetaTrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.5",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
